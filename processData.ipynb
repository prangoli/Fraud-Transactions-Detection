{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b864be6c-6973-46ed-8703-67f81f8ad832",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "This section imports all the required libraries for data processing, scaling, encoding, and visualization.\n",
    "Libraries: pandas, numpy, matplotlib.pyplot, datetime, StandardScaler, OrdinalEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffba55e-d792-4c0a-9477-29e4fca9a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f21b50e-8baa-4361-ade5-7ce2b24d3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/raw/fraudTrain.csv\").drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a450263-127d-44ae-973e-f65bf9db2041",
   "metadata": {},
   "source": [
    "# Define variables\n",
    "This section defines the categorical, continuous, target, and post-processing variables that will be used throughout the analysis.\n",
    "Variables: cat_vars, cont_vars, target, post_cont_vars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8de6cd6-dd28-4d3b-be79-4a328415ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = [\"merchant\", \"category\", \"gender\", \"city\", \"state\", \"job\"]\n",
    "cont_vars = [\"amt\", \"lat\", \"long\", \"city_pop\", \"merch_lat\", \"merch_long\"]\n",
    "target = [\"is_fraud\"]\n",
    "post_cont_vars = [\"mean_monthly_amt\", \"amount_of_monthly_trans\", \"age\", \"mean_time_between_transactions_seconds\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5818daaf-a461-4295-b177-5eab657fabff",
   "metadata": {},
   "source": [
    "# Function to process the data\n",
    "This function processes the raw data by converting date columns, extracting time-related features, calculating new features (mean monthly amount, amount of monthly transactions, age, and mean time between transactions), and merging them into the original dataset.\n",
    "Features extracted: transaction_month, transaction_day, transaction_hour, full_name, mean_monthly_amt, amount_of_monthly_trans, age, time_diff, mean_time_between_transactions_seconds.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6353bda-3be2-42d0-838f-47b04c139b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    # Convert date columns to datetime\n",
    "    data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])\n",
    "    data['dob'] = pd.to_datetime(data['dob'])\n",
    "    \n",
    "    # Extract additional time-related features\n",
    "    data['transaction_month'] = pd.DatetimeIndex(data['trans_date_trans_time']).month\n",
    "    data['transaction_day'] = pd.DatetimeIndex(data['trans_date_trans_time']).day\n",
    "    data['transaction_hour'] = pd.DatetimeIndex(data['trans_date_trans_time']).hour\n",
    "    \n",
    "    # Create full name column\n",
    "    data[\"full_name\"] = data[\"first\"] + \" \" + data[\"last\"]    \n",
    "\n",
    "    # Calculate mean monthly amount per user\n",
    "    grouped_data = data[[\"full_name\", \"transaction_month\", \"amt\"]].groupby(by=[\"full_name\", \"transaction_month\"]).sum()\n",
    "    grouped_data = grouped_data.reset_index()\n",
    "    mean_transaction_month_per_user = grouped_data.groupby(\"full_name\")[\"amt\"].mean().reset_index()\n",
    "    mean_transaction_month_per_user.columns = [\"full_name\", \"mean_monthly_amt\"]\n",
    "    \n",
    "    # Calculate amount of monthly transactions per user\n",
    "    grouped_data = data[[\"full_name\", \"transaction_month\", \"amt\"]].groupby(by=[\"full_name\", \"transaction_month\"]).count()\n",
    "    grouped_data = grouped_data.reset_index()\n",
    "    amount_transaction_hour_per_user = grouped_data.groupby(\"full_name\")[\"amt\"].sum().reset_index()\n",
    "    amount_transaction_hour_per_user.columns = [\"full_name\", \"amount_of_monthly_trans\"]\n",
    "\n",
    "    # Merge calculated features into the original dataset\n",
    "    data = data.merge(mean_transaction_month_per_user.merge(amount_transaction_hour_per_user, how='inner', on=\"full_name\"), how='inner', on=\"full_name\")\n",
    "\n",
    "    # Calculate age\n",
    "    data['Current Year'] = datetime.datetime.now().year\n",
    "    data[\"age\"] = data['Current Year'] - pd.DatetimeIndex(data['dob']).year\n",
    "\n",
    "    # Calculate time differences between transactions\n",
    "    data['datetime'] = pd.to_datetime(data['unix_time'], unit='s')\n",
    "    data.sort_values(by=['full_name', 'datetime'], inplace=True)    \n",
    "    data['time_diff'] = data.groupby('full_name')['datetime'].diff()    \n",
    "    data['time_diff_seconds'] = data['time_diff'].dt.total_seconds()\n",
    "    \n",
    "    # Calculate mean time between transactions per user\n",
    "    mean_time_between_transactions = data.groupby('full_name')['time_diff_seconds'].mean().reset_index()\n",
    "    mean_time_between_transactions.columns = ['full_name', 'mean_time_between_transactions_seconds']\n",
    "\n",
    "    # Merge calculated features into the original dataset\n",
    "    data = data.merge(mean_time_between_transactions, on=\"full_name\", how=\"inner\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec2b6d0-c1ec-42f2-a6f6-00a5a7e9dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Function to scale continuous variables\n",
    "def scale_continuous_variables(df, cont_vars):\n",
    "    scaler = StandardScaler()\n",
    "    df[cont_vars] = scaler.fit_transform(df[cont_vars])\n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "883e6c57-56ea-4ed6-bbce-33b99dbc216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode ordinal variables\n",
    "def encode_ordinal_variables(df, ord_vars):   \n",
    "    encoders = {}\n",
    "    for col in ord_vars:\n",
    "        encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        df[[col]] = encoder.fit_transform(df[[col]])\n",
    "        encoders[col] = encoder\n",
    "    return df, encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e50c32-dcbd-4efa-9e59-aa1de4e28b16",
   "metadata": {},
   "source": [
    "# Process the training data\n",
    "This section processes the training data using the previously defined functions and selects the relevant columns.\n",
    "Processing includes applying process_data, scale_continuous_variables, and encode_ordinal_variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb30049-2340-42cd-b72c-100e5c033014",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data(data)\n",
    "\n",
    "# Select relevant columns    \n",
    "data = data[cat_vars + cont_vars + post_cont_vars + target]\n",
    "\n",
    "# Scale continuous variables\n",
    "data, scaler = scale_continuous_variables(data, cont_vars + post_cont_vars)\n",
    "\n",
    "# Encode ordinal variables\n",
    "data, encoders = encode_ordinal_variables(data, cat_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d033b435-882a-4ee2-809b-624082121eff",
   "metadata": {},
   "source": [
    "# Load and process the test data\n",
    "This section loads the test data, processes it using the same steps as the training data, and selects the relevant columns.\n",
    "Test data is read from data/raw/fraudTest.csv and processed similarly to the training data.\n",
    "Apply scaling and encoding to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656bb640-ba2f-4ae8-b4d1-1164d1eb624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/raw/fraudTest.csv\").drop(columns=\"Unnamed: 0\")\n",
    "test_data = process_data(test_data)\n",
    "test_data = test_data[cat_vars + cont_vars + post_cont_vars + target]\n",
    "\n",
    "# Apply scaling and encoding to test data\n",
    "test_data[cont_vars + post_cont_vars] = scaler.transform(test_data[cont_vars + post_cont_vars])\n",
    "for col in cat_vars:\n",
    "    test_data[[col]] = encoders[col].transform(test_data[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98aae72c-4be8-4f8a-9aa4-686b1fa1282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed datasets\n",
    "data.to_csv(\"data/processed/train.csv\")\n",
    "test_data.to_csv(\"data/processed/test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
